{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification with Decision Trees and Random Forests\n",
        "\n",
        "In this notebook, we'll dive into the world of classification using two powerful machine learning algorithms: Decision Trees and Random Forests. We'll utilize the California Housing dataset from scikit-learn to predict whether a district's median house value is \"high-priced\" or not.\n",
        "\n",
        "### 1. Understanding Decision Trees and Random Forests\n",
        "\n",
        "**Decision Trees:**\n",
        "\n",
        "* **Intuitive:** Decision trees mimic human decision-making, creating a flowchart-like structure where each internal node represents a test on an attribute, each branch represents an outcome of the test, and each leaf node represents a class label or prediction.\n",
        "\n",
        "* **Interpretable:** Decision trees are easy to understand and visualize, making them ideal for explaining model decisions.\n",
        "\n",
        "* **Limitations:** Decision trees are prone to overfitting, especially when they become deep and complex.\n",
        "\n",
        "**Random Forests:**\n",
        "\n",
        "* **Ensemble Power:** Random Forests are an ensemble learning method that combines multiple decision trees to make predictions.\n",
        "\n",
        "* **Reduced Overfitting:** By averaging the predictions of multiple trees, random forests reduce the risk of overfitting and improve generalization to new data.\n",
        "\n",
        "* **Robustness:** Random forests are robust to noisy data and can handle missing values.\n",
        "\n",
        "### 2. Loading and Preprocessing the Data"
      ],
      "metadata": {
        "id": "LdUvgzSCAab0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "data = housing.data\n",
        "target = housing.target\n",
        "\n",
        "# Create a binary target variable (High-priced or not)\n",
        "median_house_value = target\n",
        "median_house_value_threshold = median_house_value.median()  # You can adjust this threshold\n",
        "target_binary = (median_house_value > median_house_value_threshold).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "82qSGc-ZAab3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "1.  **Load the dataset:** We use `fetch_california_housing` to load the dataset directly from scikit-learn.\n",
        "2.  **Create binary target:** We define a threshold for \"high-priced\" houses and create a new binary target variable indicating if the median house value is above or below this threshold.\n",
        "3.  **Split data:** We divide the data into training and testing sets for model training and evaluation.\n",
        "4.  **Standardize features:** We scale the features to improve model performance.\n",
        "\n",
        "### 3. Decision Tree Classifier\n",
        "\n",
        "**Objective:** Build a Decision Tree model to predict whether a district's median house value is \"high-priced\" or not."
      ],
      "metadata": {
        "id": "25Bfdqh9Aab5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Decision Tree Classifier: {accuracy:.2f}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier: 0.85\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWIzEinyAab5",
        "outputId": "7c051cef-b65a-48b4-d035-ad95c91f8d55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "1.  **Import necessary modules:** We import the `DecisionTreeClassifier` and `accuracy_score` from scikit-learn.\n",
        "2.  **Create the classifier:** We initialize a `DecisionTreeClassifier` object with a random state for reproducibility.\n",
        "3.  **Train the model:** We fit the classifier to the scaled training data.\n",
        "4.  **Make predictions:** We use the trained model to predict the class labels (high-priced or not) for the testing data.\n",
        "5.  **Evaluate the model:** We calculate the accuracy, which is the proportion of correct predictions.\n",
        "\n",
        "### 4. Random Forest Classifier\n",
        "\n",
        "**Objective:** Build a Random Forest model to predict whether a district's median house value is \"high-priced\" or not."
      ],
      "metadata": {
        "id": "L5G6NklmAab5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "rf_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_rf = rf_clf.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f'Accuracy of Random Forest Classifier: {accuracy_rf:.2f}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Random Forest Classifier: 0.89\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YAHovLVAab6",
        "outputId": "2a468f6d-1026-43de-d601-e652b2432053"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "1.  **Import necessary modules:** We import the `RandomForestClassifier` and `accuracy_score` from scikit-learn.\n",
        "2.  **Create the classifier:** We initialize a `RandomForestClassifier` object with 100 trees and a random state for reproducibility.\n",
        "3.  **Train the model:** We fit the classifier to the scaled training data.\n",
        "4.  **Make predictions:** We use the trained model to predict the class labels for the testing data.\n",
        "5.  **Evaluate the model:** We calculate the accuracy.\n",
        "\n",
        "**Student Challenge (Colab):**\n",
        "\n",
        "1.  **Experiment with hyperparameters:** Try changing the `max_depth` or `min_samples_split` parameters of the `DecisionTreeClassifier`. Observe how it affects the accuracy.\n",
        "2.  **Evaluate other metrics:**  Research and calculate other classification metrics like precision, recall, and F1-score for both models.\n",
        "3.  **Visualize a Decision Tree:** Use `plot_tree` from `sklearn.tree` to visualize one of the decision trees in your Random Forest.\n",
        "\n",
        "This notebook provides a hands-on introduction to classification using Decision Trees and Random Forests. By experimenting with hyperparameters and evaluating different metrics, you can gain a deeper understanding of these algorithms and their performance on real-world datasets."
      ],
      "metadata": {
        "id": "jdXcxSkPAab6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://researchleap.com/benefits-of-educational-data-mining/\">https://researchleap.com/benefits-of-educational-data-mining/</a></li>\n",
        "  <li><a href=\"https://github.com/nand-n/dolche-Backend-api\">https://github.com/nand-n/dolche-Backend-api</a></li>\n",
        "  <li><a href=\"https://lifewithdata.com/2022/07/14/a-gentle-introduction-to-decision-tree-in-machine-learning/\">https://lifewithdata.com/2022/07/14/a-gentle-introduction-to-decision-tree-in-machine-learning/</a></li>\n",
        "  <li><a href=\"https://github.com/Bodapati-Haritha/fod\">https://github.com/Bodapati-Haritha/fod</a></li>\n",
        "  <li><a href=\"https://medium.com/@ddimri/bert-for-classification-beyond-the-next-sentence-prediction-task-93acc1412749\">https://medium.com/@ddimri/bert-for-classification-beyond-the-next-sentence-prediction-task-93acc1412749</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "t7YXYGW5Aab6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}